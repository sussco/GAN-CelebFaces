{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import os\n",
    "import cv2\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize_64():\n",
    "    if not os.path.isdir(\"img_align_celeba_64\"):\n",
    "        os.mkdir(\"img_align_celeba_64\")\n",
    "    for i in range(1, 202599 + 1):\n",
    "        name = str(i).zfill(6) + \".jpg\"\n",
    "        img = cv2.imread(\"img_align_celeba/\" + name, 1)\n",
    "        img = cv2.resize(img[50:191,30:171], (64,64))\n",
    "        cv2.imwrite(\"img_align_celeba_64/\" + name, img)\n",
    "#crop_and_resize_64()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we will get the 100 000 first images and put them in a variable 178*218*3\n",
    "dataset_size = 50000 #202599\n",
    "\n",
    "        \n",
    "def get_batch(batch_size):\n",
    "        indexes = np.random.randint(1, dataset_size + 1, batch_size)\n",
    "        data = []\n",
    "        imgname = \"img_align_celeba_64/\"\n",
    "        for i in indexes:\n",
    "            #image = tf.read_file(imgname + str(i).zfill(6) + \".jpg\")\n",
    "            #image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = mpimg.imread(imgname + str(i).zfill(6) + \".jpg\")\n",
    "            image = image - np.mean(image)\n",
    "            image = image / np.var(image)\n",
    "            data.append(image)\n",
    "        return np.array(data)\n",
    "\n",
    "def sample_Z(batch_size=50, n=128):\n",
    "        return np.random.uniform(-1., 1., size=[batch_size, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_cnn(Z, fc_sizes=[110,200,300], output_dim=3, reuse=False, alpha=0.2, keep_prob=0.5,is_train=True):\n",
    "    \n",
    "    with tf.variable_scope(\"GAN/Generator_cnn\",reuse=reuse):      \n",
    "         # First fully connected layer, 4x4x512\n",
    "        fc = tf.layers.dense(Z, 4*4*512, use_bias=False)\n",
    "        fc = tf.reshape(fc, (-1, 4, 4, 512))\n",
    "        bn0 = tf.layers.batch_normalization(fc, training=is_train)\n",
    "        relu0 = tf.nn.relu(bn0)\n",
    "        drop0 = tf.layers.dropout(relu0, keep_prob, training=is_train)\n",
    "        \n",
    "        # Deconvolution, 7x7x512\n",
    "        conv1 = tf.layers.conv2d_transpose(drop0, 256, 5, 2, 'same', use_bias=False)\n",
    "        bn1 = tf.layers.batch_normalization(conv1, training=is_train)\n",
    "        relu1 = tf.nn.relu(bn1)\n",
    "        drop1 = tf.layers.dropout(relu1, keep_prob, training=is_train)\n",
    "        \n",
    "        # Deconvolution, 14x14x256\n",
    "        conv2 = tf.layers.conv2d_transpose(drop1, 128, 5, 2, 'same', use_bias=False)\n",
    "        bn2 = tf.layers.batch_normalization(conv2, training=is_train)\n",
    "        relu2 = tf.nn.relu(bn2)\n",
    "        drop2 = tf.layers.dropout(relu2, keep_prob, training=is_train)\n",
    "        \n",
    "        # Deconvolution, 14x14x256\n",
    "        conv3 = tf.layers.conv2d_transpose(drop2, 64, 5, 2, 'same', use_bias=False)\n",
    "        bn3 = tf.layers.batch_normalization(conv3, training=is_train)\n",
    "        relu3 = tf.nn.relu(bn3)\n",
    "        drop3 = tf.layers.dropout(relu3, keep_prob, training=is_train)\n",
    "        \n",
    "        # Output layer, 28x28xn\n",
    "        out = tf.layers.conv2d_transpose(drop3, output_dim, 5, 2, 'same')\n",
    "        logits = tf.tanh(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_cnn(X, fc_sizes=[110,300,200], reuse=False, alpha=0.2, keep_prob=0.5):\n",
    "    \n",
    "    with tf.variable_scope(\"GAN/Discriminator_cnn\",reuse=reuse):\n",
    "        # Input layer is 64x64x3\n",
    "        # Convolutional layer, 30x30x64\n",
    "        conv1 = tf.layers.conv2d(X, 64, 5, 2, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        lrelu1 = tf.maximum(alpha * conv1, conv1)\n",
    "        drop1 = tf.layers.dropout(lrelu1, keep_prob)\n",
    "        \n",
    "        # Strided convolutional layer, 13x13x128\n",
    "        conv2 = tf.layers.conv2d(drop1, 128, 5, 2, 'same', use_bias=False)\n",
    "        bn2 = tf.layers.batch_normalization(conv2)\n",
    "        lrelu2 = tf.maximum(alpha * bn2, bn2)\n",
    "        drop2 = tf.layers.dropout(lrelu2, keep_prob)\n",
    "        \n",
    "        # Strided convolutional layer, 5x5x256\n",
    "        conv3 = tf.layers.conv2d(drop2, 256, 5, 2, 'same', use_bias=False)\n",
    "        bn3 = tf.layers.batch_normalization(conv3)\n",
    "        lrelu3 = tf.maximum(alpha * bn3, bn3)\n",
    "        drop3 = tf.layers.dropout(lrelu3, keep_prob)\n",
    "        \n",
    "        # Strided convolutional layer, 1x1x512\n",
    "        conv4 = tf.layers.conv2d(drop3, 512, 5, 2, 'same', use_bias=False)\n",
    "        bn4 = tf.layers.batch_normalization(conv4)\n",
    "        lrelu4 = tf.maximum(alpha * bn4, bn4)\n",
    "        drop4 = tf.layers.dropout(lrelu4, keep_prob)\n",
    "        \n",
    "        # fully connected\n",
    "        flat = tf.reshape(drop4, (-1, 512))\n",
    "        out0 = tf.layers.dense(flat, 1)\n",
    "        out = tf.nn.sigmoid(out0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None,64,64,3])\n",
    "#XX = tf.reshape(X, shape=(tf.shape(X)[0], 64*64*3))\n",
    "Z = tf.placeholder(tf.float32,[None,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fake_model = generator_cnn(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data_logits = discriminator_cnn(X)\n",
    "fake_data_logits = discriminator_cnn(G_fake_model, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=true_data_logits,labels=tf.ones_like(true_data_logits)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_data_logits,labels=tf.zeros_like(fake_data_logits)))\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_data_logits,labels=tf.ones_like(fake_data_logits)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator_cnn\")\n",
    "D_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator_cnn\")\n",
    "\n",
    "G_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(G_loss,var_list = G_variables)\n",
    "D_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(D_loss,var_list = D_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\t Discriminator loss: 1.4473\t Generator loss: 0.4950\n",
      "Iterations: 1\t Discriminator loss: 1.4189\t Generator loss: 0.5197\n",
      "Iterations: 2\t Discriminator loss: 1.3848\t Generator loss: 0.5559\n",
      "Iterations: 3\t Discriminator loss: 1.3384\t Generator loss: 0.6021\n",
      "Iterations: 4\t Discriminator loss: 1.2829\t Generator loss: 0.6437\n",
      "Iterations: 5\t Discriminator loss: 1.2344\t Generator loss: 0.6697\n",
      "Iterations: 6\t Discriminator loss: 1.2051\t Generator loss: 0.6816\n",
      "Iterations: 7\t Discriminator loss: 1.1921\t Generator loss: 0.6866\n",
      "Iterations: 8\t Discriminator loss: 1.1859\t Generator loss: 0.6888\n",
      "Iterations: 9\t Discriminator loss: 1.1823\t Generator loss: 0.6897\n",
      "Iterations: 10\t Discriminator loss: 1.1786\t Generator loss: 0.6903\n",
      "Iterations: 11\t Discriminator loss: 1.1763\t Generator loss: 0.6906\n",
      "Iterations: 12\t Discriminator loss: 1.1734\t Generator loss: 0.6908\n",
      "Iterations: 13\t Discriminator loss: 1.1709\t Generator loss: 0.6910\n",
      "Iterations: 14\t Discriminator loss: 1.1690\t Generator loss: 0.6913\n",
      "Iterations: 15\t Discriminator loss: 1.1672\t Generator loss: 0.6915\n",
      "Iterations: 16\t Discriminator loss: 1.1661\t Generator loss: 0.6917\n",
      "Iterations: 17\t Discriminator loss: 1.1638\t Generator loss: 0.6919\n",
      "Iterations: 18\t Discriminator loss: 1.1620\t Generator loss: 0.6921\n",
      "Iterations: 19\t Discriminator loss: 1.1598\t Generator loss: 0.6922\n",
      "Iterations: 20\t Discriminator loss: 1.1572\t Generator loss: 0.6923\n",
      "Iterations: 21\t Discriminator loss: 1.1543\t Generator loss: 0.6924\n",
      "Iterations: 22\t Discriminator loss: 1.1509\t Generator loss: 0.6925\n",
      "Iterations: 23\t Discriminator loss: 1.1474\t Generator loss: 0.6926\n",
      "Iterations: 24\t Discriminator loss: 1.1428\t Generator loss: 0.6927\n",
      "Iterations: 25\t Discriminator loss: 1.1372\t Generator loss: 0.6927\n",
      "Iterations: 26\t Discriminator loss: 1.1314\t Generator loss: 0.6928\n",
      "Iterations: 27\t Discriminator loss: 1.1239\t Generator loss: 0.6928\n",
      "Iterations: 28\t Discriminator loss: 1.1160\t Generator loss: 0.6928\n",
      "Iterations: 29\t Discriminator loss: 1.1069\t Generator loss: 0.6928\n",
      "Iterations: 30\t Discriminator loss: 1.0978\t Generator loss: 0.6928\n",
      "Iterations: 31\t Discriminator loss: 1.0875\t Generator loss: 0.6928\n",
      "Iterations: 32\t Discriminator loss: 1.0782\t Generator loss: 0.6928\n",
      "Iterations: 33\t Discriminator loss: 1.0676\t Generator loss: 0.6928\n",
      "Iterations: 34\t Discriminator loss: 1.0598\t Generator loss: 0.6927\n",
      "Iterations: 35\t Discriminator loss: 1.0521\t Generator loss: 0.6926\n",
      "Iterations: 36\t Discriminator loss: 1.0453\t Generator loss: 0.6925\n",
      "Iterations: 37\t Discriminator loss: 1.0404\t Generator loss: 0.6924\n",
      "Iterations: 38\t Discriminator loss: 1.0354\t Generator loss: 0.6925\n",
      "Iterations: 39\t Discriminator loss: 1.0326\t Generator loss: 0.6928\n",
      "Iterations: 40\t Discriminator loss: 1.0286\t Generator loss: 0.6930\n",
      "Iterations: 41\t Discriminator loss: 1.0244\t Generator loss: 0.6930\n",
      "Iterations: 42\t Discriminator loss: 1.0224\t Generator loss: 0.6930\n",
      "Iterations: 43\t Discriminator loss: 1.0203\t Generator loss: 0.6930\n",
      "Iterations: 44\t Discriminator loss: 1.0187\t Generator loss: 0.6930\n",
      "Iterations: 45\t Discriminator loss: 1.0171\t Generator loss: 0.6929\n",
      "Iterations: 46\t Discriminator loss: 1.0163\t Generator loss: 0.6930\n",
      "Iterations: 47\t Discriminator loss: 1.0154\t Generator loss: 0.6931\n",
      "Iterations: 48\t Discriminator loss: 1.0144\t Generator loss: 0.6931\n",
      "Iterations: 49\t Discriminator loss: 1.0132\t Generator loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "batch_size = 64\n",
    "nd_steps = 10\n",
    "ng_steps = 10\n",
    "#dataset = get_data_normalized(1000)\n",
    "\n",
    "for i in range(500):\n",
    "    X_batch = get_batch(batch_size)\n",
    "    Z_batch = sample_Z(batch_size, 128)\n",
    "    _, dloss = sess.run([D_step, D_loss], feed_dict={X: X_batch, Z: Z_batch})\n",
    "    _, gloss = sess.run([G_step, G_loss], feed_dict={Z: Z_batch})\n",
    "\n",
    "    print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(i,dloss,gloss))\n",
    "\n",
    "Z_batch = sample_Z(1, 128)\n",
    "X_batch = get_batch(1)\n",
    "sess.run(G_fake_model, feed_dict={X: X_batch, Z: Z_batch})\n",
    "plt.imshow(tf.reshape(G_fake_model, shape=(64, 64,3)).eval(session=sess, feed_dict={X: X_batch, Z: Z_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_batch = sample_Z(1, 128)\n",
    "X_batch = get_batch(1)\n",
    "sess.run(G_sample, feed_dict={X: X_batch, Z: Z_batch})\n",
    "plt.imshow(tf.reshape(G_sample, shape=(64, 64,3)).eval(session=sess, feed_dict={X: X_batch, Z: Z_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_batch = sample_Z(1, 128)\n",
    "X_batch = get_batch(1)\n",
    "sess.run(G_sample, feed_dict={X: X_batch, Z: Z_batch})\n",
    "plt.imshow(tf.reshape(G_sample, shape=(64, 64,3)).eval(session=sess, feed_dict={X: X_batch, Z: Z_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_batch = sample_Z(1, 128)\n",
    "X_batch = get_batch(1)\n",
    "sess.run(G_sample, feed_dict={X: X_batch, Z: Z_batch})\n",
    "plt.imshow(tf.reshape(G_sample, shape=(64, 64,3)).eval(session=sess, feed_dict={X: X_batch, Z: Z_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
